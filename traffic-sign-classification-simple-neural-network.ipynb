{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\ndata_dir = \"/kaggle/input/traffic-sign-classification-datasets/traffic_Data/DATA\"\n","metadata":{"id":"3RQjoxQ9QMSy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndataset_path = '/kaggle/input/traffic-sign-classification-datasets/traffic_Data/DATA'\ndataset_size = sum(os.path.getsize(os.path.join(dataset_path, f)) for f in os.listdir(dataset_path))\n\nprint(f\"Dataset size: {dataset_size} bytes\")\n","metadata":{"id":"COf_abU3aVbQ","outputId":"44d6860c-acdf-4000-a1d0-5c549abe56d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir=\"/kaggle/input/traffic-sign-classification-datasets/traffic_Data/train\"","metadata":{"id":"zzGAKRt-QWXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir=\"/kaggle/input/traffic-sign-classification-datasets/traffic_Data/test\"","metadata":{"id":"rnVEvjjHUfrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\n","metadata":{"id":"wxwY2mdH9Yo0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1.0/255)#initialize train generator \n                                 \ntest_datagen = ImageDataGenerator(rescale = 1.0/255.) #initialize validation generator","metadata":{"id":"8N_xqj08-Bst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(train_dir, target_size=(128,128),batch_size=32,class_mode='categorical')\n\n#validation_generator = valid_datagen.flow_from_directory(validation_ds, target_size=(128,128),batch_size=32,class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(test_dir, target_size=(128,128),batch_size=32,class_mode='categorical')","metadata":{"id":"8ygMLhK--z4U","outputId":"be89fc41-398d-4ad3-fef8-f7dff357c859"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator[0][0].shape","metadata":{"id":"Ba75YGhnCNay","outputId":"4cbab074-7d0b-4840-8659-f62c8ed1f2d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator[0][0].shape","metadata":{"id":"WCbNtyYtCSh4","outputId":"767cc3a2-594b-4181-f16e-01bb196f621f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ntrain_dir = '/kaggle/input/traffic-sign-classification-datasets/traffic_Data/train'\n\n# Count the number of images in each subdirectory\nfor class_name in os.listdir(train_dir):\n    class_dir = os.path.join(train_dir, class_name)\n    if os.path.isdir(class_dir):\n        num_images = len(os.listdir(class_dir))\n        print(\"Class {}: {} images\".format(class_name, num_images))\n","metadata":{"id":"v8eFbt_zNLbI","outputId":"163e6798-d84c-45e3-efa3-e62c86687d4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom skimage.transform import resize\n\n# Define the path to your train folder containing the images\ntrain_dir = '/kaggle/input/traffic-sign-classification-datasets/traffic_Data/train'\n\n# Define the number of images you want to generate for each class\nnum_samples = 100\n\n# Define the parameters for data augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode='nearest'\n)\n\n# Get a list of the classes in the train folder\nclasses = sorted(os.listdir(train_dir))\n\n# Generate balanced samples for each class\nfor class_name in classes:\n    class_dir = os.path.join(train_dir, class_name)\n    num_existing_samples = len(os.listdir(class_dir))\n    if num_existing_samples < num_samples:\n        num_to_generate = num_samples - num_existing_samples\n        print(f\"Generating {num_to_generate} samples for class {class_name}\")\n        image_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir)]\n        image_array = np.array([resize(plt.imread(f), (128, 128)) for f in image_files])\n        if not os.path.exists(class_dir):\n            os.makedirs(class_dir)\n        flow = datagen.flow(image_array, batch_size=32, save_to_dir=class_dir, save_prefix=class_name, save_format='png')\n        for i in range(num_to_generate // 32):\n            flow.next()\n        remaining = num_to_generate % 32\n        if remaining > 0:\n            flow = datagen.flow(image_array, batch_size=remaining, save_to_dir=class_dir, save_prefix=class_name, save_format='png')\n            flow.next()\n    else:\n        print(f\"Class {class_name} is already balanced\")\n","metadata":{"id":"76zGlzNiNEUf","outputId":"a1f5c4c6-a4b1-416e-fe21-a2c3563c8ceb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=train_generator[0]\nimg","metadata":{"id":"knlDwq09FHnD","outputId":"4dfac760-4740-42f4-eac8-cf5d316b3264"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_generator))\nprint(len(test_generator))","metadata":{"id":"0ZCjad6bCpYe","outputId":"79c91d16-5764-4bb2-a486-caea401f003b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_generator)","metadata":{"id":"vvlamPcvC1Cs","outputId":"0b8dbec0-9bdd-4628-f46d-6d184b681124"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nlabels = [k for k in train_generator.class_indices]\nsample_generate = train_generator.__next__()\n\nimages = sample_generate[0]\ntitles = sample_generate[1]\nplt.figure(figsize = (20 , 20))\n\nfor i in range(15):\n    plt.subplot(5 , 5, i+1)\n    plt.subplots_adjust(hspace = 0.3 , wspace = 0.3)\n    plt.imshow(images[i])\n    plt.title(f'Class: {labels[np.argmax(titles[i],axis=0)]}')\n    plt.axis(\"off\")","metadata":{"id":"64ZFMi8UC5su","outputId":"25779bfa-c171-43ff-f88f-6f95642bc148"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom skimage import io\n\n# Load image\nimg_url = \"/kaggle/input/traffic-sign-classification-datasets/traffic_Data/test/10/010_0001.png\"\nimg = io.imread(img_url)\n\n# Display image\nplt.imshow(img)\nplt.axis('off')\nplt.show()","metadata":{"id":"9LDt5GarDgeE","outputId":"f661d649-5218-4343-aca2-a6437db4da52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Activation, Flatten, Dropout, Dense","metadata":{"id":"W2JbEp09GC39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Softmax\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"id":"iV74JyfNGI7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_model = Sequential([ \n    Flatten(input_shape=(128,128,3), name='input_layer'),\n    Dense(64, activation='relu', name='layer1'),\n    # Dense(64, activation='relu', name='layer2'),\n    Dense(32, activation='relu', name='layer3'),\n    # Dense(32, activation='relu', name='layer4'),\n    Dense(58, activation='softmax', name='output_layer')\n])","metadata":{"id":"f1jXuTfzGLIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_model.summary()","metadata":{"id":"bMh2TMyTGWo6","outputId":"3973d7ab-7d73-4958-e17d-5d5d9558f764"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(data_dir)","metadata":{"id":"_tUFGDXfGhZR","outputId":"0e424a91-4331-48a1-8501-607b49cf407b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\nseq_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"id":"8S9PyuxEGjzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = train_generator[0]\nprint(img[0].shape) # shape of the input batch\nprint(img[1].shape) # shape of the target labels","metadata":{"id":"8J-h_vhYEV6G","outputId":"4de357cd-b0c3-423a-8941-2457ee179453"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"har = seq_model.fit(train_generator,\n                    epochs=5,\n                    validation_data=test_generator)","metadata":{"id":"0gybUj2HGnBr","outputId":"7e95537d-a10c-4718-f6e6-832970b03886"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_categories = len(os.listdir('/content/gdrive/MyDrive/traffic_Data/train'))# number of categories\nprint(n_categories)","metadata":{"id":"G_uTuSO8E6yf","outputId":"8ede4c77-5a68-4952-bd9f-14bf1b0bdccf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nfrom keras.preprocessing.image import ImageDataGenerator\nimport os","metadata":{"id":"2y8BMKubF_SP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_list = os.listdir(test_dir)\nnum_files = len(file_list)\nbatch_size = 32\nnum_batches = int(num_files / batch_size)","metadata":{"id":"KNTuofvZGC3b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame(har.history)\nresults.tail()","metadata":{"id":"ddAQxCuzFhzn","outputId":"a0fe5cc1-9281-4d76-ec28-34f74d131374"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame(har.history)\nresults.head()","metadata":{"id":"7c5HHFaq89Jb","outputId":"ad5edd67-c753-4a8c-8355-e8c2635c56ec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# summarize history for accuracy\nplt.plot(har.history['accuracy'])\nplt.plot(har.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(har.history['loss'])\nplt.plot(har.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"id":"pNZZT-W4-MXp","outputId":"595c5f1a-f652-414d-fccf-509c4033e51f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloss_plot = results.plot(y='loss', title='Loss vs Epochs', legend=False)\nloss_plot.set(xlabel='Epochs', ylabel='Loss')","metadata":{"id":"VCuzWK1V-uQt","outputId":"30ff6f0a-5072-457e-831b-104c4ae0713c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_plot = results.plot(y='accuracy', title='Accuracy vs Epochs', legend=False)\nacc_plot.set(xlabel='Epochs', ylabel='Accuracy')","metadata":{"id":"gLLyJTHt-0vf","outputId":"c30e7512-11d5-4d90-fe38-7dce5f6b652a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_1 = test_generator.classes\ny_pred_1 =seq_model.predict(test_generator)\ny_pred_1 = np.argmax(y_pred_1,axis=1)","metadata":{"id":"tZHn7t1OHqYi","outputId":"10b2e54a-f1d0-449a-bef3-84c7d3c0148a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = seq_model.evaluate(test_generator)","metadata":{"id":"2Wt5DvqvHvXA","outputId":"7d9a8f0e-44d8-4546-d3dd-9b9b03debc81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport cv2\n\nt=cv2.imread('/content/gdrive/MyDrive/traffic_Data/test/0/000_0011.png')\nplt.imshow(t)","metadata":{"id":"SQ5CUhYD-63p","outputId":"71282165-de06-4e90-ad0d-82a60dc88e69"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import img_to_array\ntestimg=cv2.resize(t,(128,128))\ntestimg=img_to_array(testimg)/255\nh=np.expand_dims(testimg,axis=0)\nr=seq_model.predict(h)\nclassnames=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n            \"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\n            \"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\n            \"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\n            \"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"\n            \"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\"]\nypred=classnames[np.argmax(r)]\nypred","metadata":{"id":"C0ikp3g4ph_q","outputId":"445f7e67-62a3-4791-c2f4-23d141d0849c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\ntest=ImageDataGenerator(rescale=1./255)","metadata":{"id":"JkTB1ka3hM8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pathlib","metadata":{"id":"CiO4FFAwhZVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n            \"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\n            \"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\n            \"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\n            \"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"\n            \"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\"]\nimg_size = 128\ndef get_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","metadata":{"id":"tYnidQs4ha7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimage_directory = '/content/gdrive/MyDrive/traffic_Data/test/1'\nimg_size = 128\n\nimages = [] \nfor filename in os.listdir(image_directory):\n    path = os.path.join(image_directory, filename)\n    img = Image.open(path)\n    img = img.resize((img_size, img_size))\n    images.append(img)\n\nimages = np.array([np.array(img) for img in images])\nimages = images / 255.0\n\npredictions =seq_model.predict(images)\n\n# Select image to display\nimg_index = 0\n\n# Get predicted class label\nclass_label = np.argmax(predictions[img_index])\n\n# Display image and predicted class label\nplt.imshow(images[img_index])\nplt.axis('off')\nplt.title('Predicted class: ' + str(class_label))\nplt.show()","metadata":{"id":"OR9qAL_JNKsY","outputId":"fe94afdb-30d2-4055-c2bf-dd862ae7f478"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(images)):\n    predicted_class = np.argmax(predictions[i])\n    class_probability = predictions[i, predicted_class]\n    print(f'Predicted class for {i+1}.jpg : {labels[predicted_class]}')\n    print('Class probability:', class_probability)","metadata":{"id":"gaXN3tkAN7-R","outputId":"74b7df45-debc-4d99-f4e9-89c6ef45b272"},"execution_count":null,"outputs":[]}]}