{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\n\ndata_dir = \"/kaggle/input/traffic-sign-classification-datasets/traffic_Data/DATA\"\n","metadata":{"id":"3RQjoxQ9QMSy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndataset_path = '/kaggle/input/traffic-sign-classification-datasets/traffic_Data/DATA'\ndataset_size = sum(os.path.getsize(os.path.join(dataset_path, f)) for f in os.listdir(dataset_path))\n\nprint(f\"Dataset size: {dataset_size} bytes\")\n","metadata":{"id":"COf_abU3aVbQ","outputId":"2fb43a1d-6af0-4f0b-d452-24b2be95eb1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir=\"/kaggle/input/traffic-sign-classification-datasets/traffic_Data/train\"","metadata":{"id":"zzGAKRt-QWXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir=\"/kaggle/input/traffic-sign-classification-datasets/traffic_Data/test\"","metadata":{"id":"rnVEvjjHUfrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\n","metadata":{"id":"wxwY2mdH9Yo0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1.0/255)#initialize train generator \n                                 \ntest_datagen = ImageDataGenerator(rescale = 1.0/255.) #initialize validation generator","metadata":{"id":"8N_xqj08-Bst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(train_dir, target_size=(128,128),batch_size=32,class_mode='categorical')\n\n#validation_generator = valid_datagen.flow_from_directory(validation_ds, target_size=(128,128),batch_size=32,class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(test_dir, target_size=(128,128),batch_size=32,class_mode='categorical')","metadata":{"id":"8ygMLhK--z4U","outputId":"47749c7d-67a3-4098-edbb-f030ee19eb74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ntrain_dir = '/kaggle/input/traffic-sign-classification-datasets/traffic_Data/train'\n\n# Count the number of images in each subdirectory\nfor class_name in os.listdir(train_dir):\n    class_dir = os.path.join(train_dir, class_name)\n    if os.path.isdir(class_dir):\n        num_images = len(os.listdir(class_dir))\n        print(\"Class {}: {} images\".format(class_name, num_images))\n","metadata":{"id":"v8eFbt_zNLbI","outputId":"2ed75cb6-f070-49be-b3d2-4cc4189e3a2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom skimage.transform import resize\n\n# Define the path to your train folder containing the images\ntrain_dir = '/kaggle/input/traffic-sign-classification-datasets/traffic_Data/train'\n\n# Define the number of images you want to generate for each class\nnum_samples = 100\n\n# Define the parameters for data augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode='nearest'\n)\n\n# Get a list of the classes in the train folder\nclasses = sorted(os.listdir(train_dir))\n\n# Generate balanced samples for each class\nfor class_name in classes:\n    class_dir = os.path.join(train_dir, class_name)\n    num_existing_samples = len(os.listdir(class_dir))\n    if num_existing_samples < num_samples:\n        num_to_generate = num_samples - num_existing_samples\n        print(f\"Generating {num_to_generate} samples for class {class_name}\")\n        image_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir)]\n        image_array = np.array([resize(plt.imread(f), (128, 128)) for f in image_files])\n        if not os.path.exists(class_dir):\n            os.makedirs(class_dir)\n        flow = datagen.flow(image_array, batch_size=32, save_to_dir=class_dir, save_prefix=class_name, save_format='png')\n        for i in range(num_to_generate // 32):\n            flow.next()\n        remaining = num_to_generate % 32\n        if remaining > 0:\n            flow = datagen.flow(image_array, batch_size=remaining, save_to_dir=class_dir, save_prefix=class_name, save_format='png')\n            flow.next()\n    else:\n        print(f\"Class {class_name} is already balanced\")\n","metadata":{"id":"76zGlzNiNEUf","outputId":"a1f5c4c6-a4b1-416e-fe21-a2c3563c8ceb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=train_generator[0]\nimg","metadata":{"id":"knlDwq09FHnD","outputId":"a191dced-1d71-4093-c30c-a6413bf1a6e1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Activation, Flatten, Dropout, Dense","metadata":{"id":"W2JbEp09GC39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Softmax\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"id":"iV74JyfNGI7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\ndef vgg19_model(input_shape=(128, 128, 3), num_classes=58):\n    model = Sequential()\n\n    # Block 1\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Block 2\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Block 3\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Block 4\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Block 5\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n    # Classification block\n    model.add(Flatten())\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dense(4096, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    return model\n","metadata":{"id":"f1jXuTfzGLIL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create the model\nmodel = vgg19_model()\n\n# print the model summary\nmodel.summary()","metadata":{"id":"bMh2TMyTGWo6","outputId":"56856b9a-ff97-490a-a548-559270178217"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(data_dir)","metadata":{"id":"_tUFGDXfGhZR","outputId":"f7139511-2bc5-4e65-d1c0-dae525e100d0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])  ","metadata":{"id":"8S9PyuxEGjzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"har = model.fit(train_generator, validation_data=test_generator, epochs=5,batch_size=32)","metadata":{"id":"RxWiWosegTsB","outputId":"ca3f5ebe-104a-4b05-c655-1e8c8f2a27dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np ","metadata":{"id":"hYYUbQ9K7BVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nimport os","metadata":{"id":"PwF4rT4T7CoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_list = os.listdir(test_dir)\nnum_files = len(file_list)\nbatch_size = 32\nnum_batches = int(num_files / batch_size)","metadata":{"id":"fQbJVTt_7FFv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"id":"yOR525If86Rz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(har.history)\ndf.head()","metadata":{"id":"7c5HHFaq89Jb","outputId":"4c237a37-228e-4b93-d1b7-2291ea8ace66"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# summarize history for accuracy\nplt.plot(har.history['accuracy'])\nplt.plot(har.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(har.history['loss'])\nplt.plot(har.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"id":"pNZZT-W4-MXp","outputId":"577e414f-9e1e-4d3d-9044-bd7379adae84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.SGD(learning_rate=0.001)\nacc = tf.keras.metrics.SparseCategoricalAccuracy()\nlos = tf.keras.losses.SparseCategoricalCrossentropy()\n\n\nmodel.compile(\n    \n    optimizer = opt,\n    loss = los,\n    metrics=acc\n)","metadata":{"id":"sA5T3tV4-lT7","outputId":"07378aa3-669f-4e27-ffaf-1e117a1a818e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nloss_plot = df.plot(y='loss', title='Loss vs Epochs', legend=False)\nloss_plot.set(xlabel='Epochs', ylabel='Loss')","metadata":{"id":"VCuzWK1V-uQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_plot = df.plot(y='accuracy', title='Accuracy vs Epochs', legend=False)\nacc_plot.set(xlabel='Epochs', ylabel='Accuracy')","metadata":{"id":"gLLyJTHt-0vf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport cv2\n\nt=cv2.imread('/kaggle/input/traffic-sign-classification-datasets/traffic_Data/test/0/000_0011.png')\nplt.imshow(t)","metadata":{"id":"SQ5CUhYD-63p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import img_to_array\ntestimg=cv2.resize(t,(128,128))\ntestimg=img_to_array(testimg)/255\nh=np.expand_dims(testimg,axis=0)\nr=model.predict(h)\nclassnames=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n            \"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\n            \"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\n            \"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\n            \"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"\n            \"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\"]\nypred=classnames[np.argmax(r)]\nypred","metadata":{"id":"C0ikp3g4ph_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\ntest=ImageDataGenerator(rescale=1./255)","metadata":{"id":"JkTB1ka3hM8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pathlib","metadata":{"id":"CiO4FFAwhZVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n            \"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\n            \"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\n            \"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\n            \"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\"\n            \"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\"]\nimg_size = 224\ndef get_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","metadata":{"id":"tYnidQs4ha7M"},"execution_count":null,"outputs":[]}]}